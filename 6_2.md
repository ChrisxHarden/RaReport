# June 2 Report


## TODO:
1. jukebooks
2. rq-va
3. ImageBart
4. parti
5. score-based model in latent space
   
6. Mask-Git
7. VQGAN-CLIP
8. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors
9.  Fast decoding in sequence models using discrete latent variables.
10. Latent video transformer.
11. Videogpt: Video generation using vq-vae and transformers.
12. Feature quantization improves gan training
13. uvim


## Done:
1. Dlformer
2. High-Resolution Video Synthesis with Latent Diffusion Models
3. Disentanglement via Latent Quantization's latent quantizaion part

## Doing:
1. Autoregressive Image Generation Using Residual Quantization




----------------------

## quetions to be asked:
1. This is not so relevant to my task. The take-home challenge for us which we need to perform dataset distillation on textdata. When we want to optimize the tokens in the synthetic dataset, we encounter the problem of datatype transformation, i.e. to optimize the data which is the id of tokens, we need to transform them to floating number and finally transform them back to valid data.  Can straight-through gradient estimator help with this? What I am thinking is use this for the undifferentiable part of this process. Maybe we still need to transform the datatype, but we can let gradient flow through this step.
2. for qlae, can I skip the infoMEc?


   











------------------------
## Conclusion(quick notes) on papers
### DLFormer: Discrete Latent Transformer for Video Inpainting
video inpainting
learn a video-specific and discriminative codebook as well as the corresponding autoencoder to represent the target video in the discrete latent space


develop a residual temporal aggregation block to relieve temporal visual jitters caused by the discrete prediction across adjacent frames.


However, we can not directly utilize the VQ-VAE since that there is no ground truth for missing regions. Therefore, we extend the VQ-VAE to learn the discrete latent representation for the corrupted video sequence.(only learnt on corrupted frames)

propose a dynamic codebook refining scheme where for each video we maintain a codebook with rich context and video-specific information.

employ a much more general codebank with 8192 prototype vectors pretrained from a large-scale dataset and customize it to a specific video sequence. namely for a specific video, its own code book will only be those ever occured in the embedded sequence of that video.

fill unseen regions with completion token. encode spatial token as well


propose a self-supervised transformer framework to learn code constituent distribution in valid regions. Specifically, we randomly generate mask mrto corrupt the valid region and thus form a pseudo unseen region


### Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models
The encoding part stays the same as this article aims to utilize the already existed LDM models for video synthesis. They modified the latent diffusion model by inserting temporal layers after spatial layers and fix the spatial layer, only training those temporal layers so that the pre-trained DMs for fixed image can be reused. During training, the spatial layers interpret the video as a batch of independent images (by shifting the temporal axis into the batch dimension), and for each temporal mixing layer, reshape back to video dimensions. They also introduce additional temporal layers for the autoencoder’s decoder, which is finetuned on video data with a (patch-wise) temporal discriminator built from 3D convolutions. It also applied other techniques for longer and higher resolution video producing, but it's not in our interest.

### Disentanglement via Latent Quantization
Target of Disentanglement representation learning:
Given a dataset of paired source-data samples {(s, x = g(s))} from the nonlinear ICA model (1), learn an encoder ˆ g−1: X → Z and decoder ˆ g : Z → X solely using the data {x} such that (i) the InfoMEC as estimated from samples {(s, z = ˆ g−1◦ g(s))} from the joint source-latent distribution is high, while (ii) maintaining an acceptable level of reconstruction error between x and ˆ g ◦ ˆ g−1(x)

Namely: we want the latents to better represent the source variable and we want the encoder-decoder combination can better keep the structure of observed variable x.

latents here do not essentially reduce dimensionality of x




using a compositional latent space. aka latent quantizaion


Vector quantization maps continuous latents (black dots) to their nearest codes (circles); each code’s position is optimized separately over the course of training.



Latent quantization maps latents onto a regularly-structured grid in which only the per-dimension codebooks (orange and turquoise ticks) are optimized



Instead of quantizaing the latent space with high-dimension vectors, they use the Cartesian product of nz distinct scalar codebooks: Z = V1× · · · × Vnz, where each codebook, Vj, is a set of nv reals.

encourage our model to disentangle

the nearest neighbor calculation can be done elementwise, which is highly efficien

We motivate the use of strong model regularization with the conjecture that, of all the theoretically possible mappings from compositional latent space to data, the most parsimonious will be the true generative model or something close enough to it. We operationalize this by using a high weight decay on both the encoder and decoder networks













--------------------------
## Quick Notes

1. Fast decoding in sequence models using discrete latent variables.
2. Latent video transformer.
3. Videogpt: Video generation using vq-vae and transformers.
4. Feature quantization improves gan training
5. 