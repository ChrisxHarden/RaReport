# HomeWork 1

## 1
Use $det(A-\lambda I)=0$, solve $\lambda$.
$\lambda=\frac{5\pm \sqrt 5}{2}$

$\lambda$ are the eigenvalues, then solve the eigenvectors to be $[1,\frac{\sqrt{5}-1}{2}]^T;[1,\frac{-\sqrt{5}-1}{2}^T]$. If we normalize them, we get:
$[0.85065081,0.52573111]^T,[-0.52573111,0.85065081]^T$ as eigenvectors.

To test in python, we need to use numpy.linalg.eig() function.

![pic for 1](https://github.com/ChrisxHarden/RaReport/blob/dbfdfc6861d7e959394e1238db4169e2ab0bde12/homework/2-1.png)

## 2
### (a):
Suppose we have $A$ and $B$ as symmetric matrices, $(A+B)^T=A^T+B^T=A+B$. Thus the sum of two symmetric matrices is a symmetric matrix.

### (b):
Suppose we have $A$ and $B$ as symmetric matrices, $(A+B)^T=A^T+B^T=-A-B=-(A+B)$. Thus the sum of two skew-symmetric matrices is a skew-symmetric matrix.

## 3
### (a): 
$A\cup B\cup C$

![pic for 3(a)](https://github.com/ChrisxHarden/RaReport/blob/main/homework/1.png)

Red means the required event.

### (b):
$\neg((A \cap B)\cup(A\cap C)\cup (B\cap C))$

![pic for 3(a)](https://github.com/ChrisxHarden/RaReport/blob/main/homework/2.png)

Red means the required event.Blue part is to demonstrate the negative of the required event.

### (c):
$\neg (A\cup B\cup C)$

![pic for 3(a)](https://github.com/ChrisxHarden/RaReport/blob/main/homework/3.png)

Red means the required event.Blue part is to demonstrate the negative of the required event.

### (d):
$A \cap B \cap C$

![pic for 3(a)](https://github.com/ChrisxHarden/RaReport/blob/main/homework/4.png)

Red means the required event.


### (e):
$(A\cap \neg B \cap \neg C)\cup (\neg A\cap  B \cap \neg C)\cup (\neg A\cap \neg B \cap C)$

![pic for 3(a)](https://github.com/ChrisxHarden/RaReport/blob/main/homework/5.png)

Red means the required event.

### (f):
$A\cap B\cap \neg C$

![pic for 3(a)](https://github.com/ChrisxHarden/RaReport/blob/main/homework/6.png)

Red means the required event.

## 4
While we organize the path of a moving robot in a closed room where the robot is the only moving stuff, we can know what the room will be like $S_{t+1}$ at the next timestep given current room state $S_{t}$ and robot's action $a_t$. 

I think it's valid. As the robot is the only moving object in the closed room, we can modelize the room as somewhat stationary scene and the state of the room will only be changed due to the moving robot. While predicting the next state of the room, only the robot's action can affect it.

## 5
We can predict next hour's weather by only looking at current weather. If it rains, we can predict next hour probably gonna be rain. 

I don't think it's necessarily valid since weather is very complex and can be affected by many reason, we can always witness the sudden rain drop in summer or even hailstone hitting the ground. 

## 6
$a>0$

$\int_{-\infty}^{\infty}f_{X}(x)=1=a*arctan(e^x)|^{\infty}_{-\infty}=\frac{a\pi}{2}$

$a=\frac{2}{\pi}$

## 7
### (a)
$P=E{(x_1-\bar{x_1})(x_2-\bar{x_2})^T}$

As $x_1$ and $x_2$ are independent, $(x_1-\bar{x_1})$ and $(x_2-\bar{x_2})$ are also independent, so we can get:

$E{(x_1-\bar{x_1})(x_2-\bar{x_2})^T}=E{(x_1-\bar{x_1})}E{(x_2-\bar{x_2})^T}=(E{(x_1)}-\bar{x_1})(E{(x_2)^T}-\bar{x_2}^T)=0$

So, independent random vectors are uncorrelated.

### (b)
We name the the two Gaussain random vectors as $x_1$ and $x_2$, the joint distribution will be $f(x_1,x_2)$. Iff $x_1$ and $x_2$ are independent, $f(x_1,x_2)=f(x_1)f(x_2)$

Suppose $x_1$ and $x_2$ have $k_1$ and $k_2$ dimensions separately, we can then express $f(x_1,x_2),f(x_1) and f(x_2)$ as :

$f(x_1,x_2)=\frac{1}{(2\pi)^{\frac{k_1+k_2}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$

$f(x_1)=\frac{1}{(2\pi)^{\frac{k_1}{2}}|\Sigma_1|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_1-\mu_1)^T\Sigma_1^{-1}(x_1-\mu_1))$

$f(x_2)=\frac{1}{(2\pi)^{\frac{k_2}{2}}|\Sigma_2|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_2-\mu_2)^T\Sigma_2^{-1}(x_2-\mu_2))$


Here, $\mu$ and $\Sigma$ represent the mean and covariance matrix of the joint distribution, $\mu_1$ and $\Sigma_1$ represent the mean and covariance matrix of $x_1$,$\mu_2$ and $\Sigma_2$ represent the mean and covariance matrix of $x_2$


As $\Sigma_1$ and $\Sigma_2$ are covariance matrices of $x_1$ and $x_2$, they are symmetric and therefore their determinant $|\Sigma_1|,|\Sigma_2|$ are the product of their eigenvalues. $|\Sigma_1|=\prod_{i=1}^{k_1}\lambda_1^i,|\Sigma_2|=\prod_{i=1}^{k_2}\lambda_2^i$

Since $x_1$ and $x_2$ are uncorrelated gaussian random vectors, the off-diagonals of the covariance matrix are 0 and the diagonals are the eigenvalues of $\Sigma_1$ and $\Sigma_2$, therefore $|\Sigma|=\prod_{i=1}^{k_1}\lambda_1^i*\prod_{j=1}^{k_2}\lambda_2^j|=|\Sigma_1|*|\Sigma_2|$. We can get :

$\frac{1}{(2\pi)^{\frac{k_1+k_2}{2}}|\Sigma|^{\frac{1}{2}}}=\frac{1}{(2\pi)^{\frac{k_1}{2}}|\Sigma_1|^{\frac{1}{2}}}*\frac{1}{(2\pi)^{\frac{k_2}{2}}|\Sigma_2|^{\frac{1}{2}}}$

If we take $x$ as the concatenation of $x_1, x_2$, then $\mu$ is also the concatenation of $\mu_1, \mu_2$, $i.e. x=[x_1^1,x_1^2,...x_1^{k_1},x_2^1,x_2^2,...x_2^{k_2}],\mu=[\mu_1^1,\mu_1^2,...\mu_1^{k_1},\mu_2^1,\mu_2^2,...\mu_2^{k_2}]$, as we get the off-diagonals of the covariance matrix $\Sigma$ are 0, we know that each component in $x_1$ and $x_2$ are also uncorrelated therefore $\Sigma_1$,$\Sigma_2$ are also matrices with the off-diagonals of the covariance matrix being 0 and the diagonals being the eigenvalues. So we get:

$(x-\mu)^T\Sigma^{-1}(x-\mu)=(x_1^1-\mu_1^1)^2/\lambda_1^1+(x_1^2-\mu_1^2)^2/\lambda_1^2+...+(x_1^{k_1}-\mu_1^{k_1})^2/\lambda_1^{k_1}+(x_2^1-\mu_2^1)^2/\lambda_2^1+(x_2^2-\mu_2^2)^2/\lambda_2^2+...+(x_2^{k_2}-\mu_2^{k_2})^2/\lambda_2^{k_2}$

$=\sum_{i=1}^{k_1}(x_1^i-\mu_1^i)^2*\frac{1}{\lambda_1^i}+\sum_{j=1}^{k_2}(x_2^j-\mu_2^j)^2*\frac{1}{\lambda_2^j}=(x_1-\mu_1)^T\Sigma_1^{-1}(x_1-\mu_1)+(x_2-\mu_2)^T\Sigma_2^{-1}(x_2-\mu_2)$

We then use
$(x-\mu)^T\Sigma^{-1}(x-\mu)=(x_1-\mu_1)^T\Sigma_1^{-1}(x_1-\mu_1)+(x_2-\mu_2)^T\Sigma_2^{-1}(x_2-\mu_2)$ and $\frac{1}{(2\pi)^{\frac{k_1+k_2}{2}}|\Sigma|^{\frac{1}{2}}}=\frac{1}{(2\pi)^{\frac{k_1}{2}}|\Sigma_1|^{\frac{1}{2}}}*\frac{1}{(2\pi)^{\frac{k_2}{2}}|\Sigma_2|^{\frac{1}{2}}}$ to get:

$f(x_1,x_2)=\frac{1}{(2\pi)^{\frac{k_1+k_2}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$

$=\frac{1}{(2\pi)^{\frac{k_1}{2}}|\Sigma_1|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_1-\mu_1)^T\Sigma_1^{-1}(x_1-\mu_1))*\frac{1}{(2\pi)^{\frac{k_2}{2}}|\Sigma_2|^{\frac{1}{2}}}exp(-\frac{1}{2}(x_2-\mu_2)^T\Sigma_2^{-1}(x_2-\mu_2))=f(x_1)*f(x_2)$

We can prove $x_1$ and $x_2$ are independent random vectors.

