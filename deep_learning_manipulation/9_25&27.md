## GIFT: Generalizable Interaction-aware Functional Tool Affordances without Labels

**Keypoints:sampling based method,**



1. fit an object's affordances and the demands of the task
2. Visual affordance learnign can be benefitted form goal-directed interaction experience, current method need to rely on human label or expert policies.
3. The affordance in this method is based on physical interacions, and don't need human label or expert policies. Performing a self-supervised learning
4. Has 2 phases: 1） discover visual affordances from goal-directed interaction with a set of procedurally generated tools， 2）train a model to predict new instances of the discovered affordances on novel tools in a self-supervised fashion
5. uses a sparse keypoint representation to predict grasp and interaction points for different tasks.
6. affordances: the action possibilities offered by an object

defining affordances only based on whether an action can be completed with a particular tool, without imposing explicit constraints on the object’s appearance, physical, or geometric properties. Simply put, a hammer can be defined as any object that “affords” the action of hammering.

To test the affordance, we can just perform the action and observe the result. however, in reality the space of possible action is too large to try and translate an affordance representation to corresponding action is also hard.

7. Prior work has avoided this roadblock in two ways: (1) with human supervision or (2) by greatly constraining the space of possible actions. First one can be expensice, time-consuming to collect dataset, may encode irrelevent human biases. Second one will limits the set of learnable affordances.

8. This paper: By **efficiently generating trajectories with a simple sampling-based method ** and recovering affordances from contact data, we can achieve representation learning without human supervision and without constraining the space of possible manipulation actions.
   
### The method has 3 stages
#### First: collect experience conditioned on sampled grasp and interaction keypoints.
1.  First: collect experience conditioned on sampled grasp and interaction keypoints. The keypoints are detected by SparseKP net from an RGBD image which are considered the candidate keypoints to grasp or to interact with the world. Then sample grasp and interaction keypoints to perfrom a stable grasp by grasping near the grasp point then sample the rest trajectory with MPPI.using a reward that encodes success and encourage using the interaction keypoint
2.  Sample a point cloud from the D channel of RGB-D observation of the tool. Masking out the most common value considering them as the table. Using PointNet to encode the point's feature and ResNet to extract the feature from RGB channel corresponding to the point and concanate them together. Later pass to a final module that predicts keypoints.
3.  The keypoint prediction network is inspired by 6-Pack. With this paper's goal is to detect keypoints that represent tool geometery and can be possible interaction and grasping point. Namely: keypoints that are spread across the tool and lie near geometric features, such as edges and corners. To this end we choose a different combination of losses: Quadrtic loss and coverage loss.(This kind of introducing a human bias, but I support this bias here as normally when human operates only on the surface?)
4.  Coverage loss: It means to have the keypoints spread all over the tool to avoid the keypoints too concentrated. (Why not using distance between point pairs) For every keypoint, it induces a distribution over all vertices of the mesh which is inversely proportionate to the distance between the vertex and the keypoint. Also, they want all distribution's average to be uniform, meaning each vertex has similar avearge distance to keypoints. They also take the geometry of tool into consideration by minimizaing earth-mover distance(EMD) using sinkhorn distance as its differentilable approxiamtion.
5.  Quadric loss:The quadric loss encourages keypoints to be near edges and corners of the tool’s surface, since we expect keypoints near edges and corners to better capture the geometric features of the tool. It's based on quadric error which measures the distance between a point and a plane.
6. Grasp planning:Use a pretrained network (FC-GQ-CNN) which is an extension of Dexnet 4.0. As wanting to grasp the grasping point, the input is a crop of RGB-D image with grasping as the center. The Dexnet then generates 50 canidate grasps and they choose the actuall grasp which is highly possible and closest to the grasping point .
7. Motion planning: Given the predicted keypoints, K, we uniformly sample a provisional interaction keypoint $x^{inter}$. Planning begins with a base trajectory of length H with all actions set to 0. The action space is the change in $(x, y, z)$ position and z-axis rotation of the end effector. Then generate a set of M perturbed trajectories, by adding Gaussian noise to the base trajectory. The perturbed trajectories are executed in the simulator and scored using the reward function.The reward-weighted average of these trajectories becomes the plan and execute its first step before the whole process repeats.Once they have computed the trajectory, they discard the provisional interaction point $x^{inter}$, and replace it with the keypoint closest to the first point of contact between the tool and the target in the sampled trajectory. The result of this procedure is a tuple (K, xgrasp, xinter, R), which is added to the training set.
8. The reward function of Motion planning: Task completion and the distance of the provisional interaction point to the target object. For each task family considered, they define a task-specific performance reward that is shared across all instances (i.e., tools) in that family while the rest of the framework remains unchanged between tasks. 

#### extract affordance from the sampled trajectories
1.  Next: extract affordance from the sampled trajectories which becomes the training examples for affordance model

#### Using the extracted affordance to train an affordance model
1.  Final: Using the extracted affordance to train an affordance model. THe model is GNN with its input being the graph defined over the detected keypoints. The model calculates the probability distribution over all keypoints pairs with them serving as grasping and interaction points. The weights of the model are optimized so that high probability is assigned to keypoint pairs that are compatible with a trajectory that allows task completion, thereby providing the desired affordance. In this way, the model implicitly learns how to map tool geometry to an affordance, and evaluate that affordance on new tool geometries.
2. The GNN consists of three generalized graph convolution layers, each followed by a SAGPool layer.The graph convolutions layers propagate information through the graph. The pooling layers incrementally aggregate information into a global encoding. The penultimate layer of the GNN computes local feature encodings at each node, and a single global encoding for the entire graph. They also build pairwise features by appending two local node encodings to the global encoding, and passing the result through a linear layer which scores that particular choice of nodes as grasp and interaction keypoints.
3. This GNN computes a probability distribution over pairs of keypoint indices. They want to find model weights that maximize the expected reward of trajectories matching grasp and interaction keypoints sampled under this output distribution D. Our optimization objective is the REINFORCE loss.


