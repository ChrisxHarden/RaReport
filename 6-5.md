# June 4 Report

## TODO:
1. Find papers connected with or using residual quantization
2. Find papers connected with QLAE


## Done:
1. MASKGIT
2. Autoregressive Image Generation using Residual Quantization
3. VQ-GAN-CLIP
4. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors
5. UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes


### Doing:
1. STAR(reading)
2. PARTI(moving to overleaf)

---------------------------------------------
## Questions:








----------------------------------------------
### MAsk git
the first step i.e. tokenization remains the same as VQ-GAN

used a Masked Visual Token Modeling training paradigm
at each step it will sample of subset of the tokenized input image and replace with mask token. Then feed the masked token into a bi-directional transformer to predict the masked token. Due to the conditional dependency in MVTM has two directions, allows image generation to utilize richer contexts by attending to all tokens in the image.

It also applys an iterative decoding schedule to boost the synthesis speed. At each iteration, it will predict the probabilities of tokens from the codebook at each masked position and replace them with the token that has the highest possibility. A prediction score for that position will be calculated based on the probability. Then by a mask scheduling function, the positions having the prediction score lower than the threshold given by the mask scheduling function will be masked again while the other positions will have its prediction score set to 1. This can guarantee the model stop at certain steps regardless of the input size.

### Autoregressive Image Generation using Residual Quantization
using residual quantization instead of vector quantization to create latent space. To best store the details of the input map, VQ will need to preserve a huge codebook which leads to the increase of model parameters and the codebook collapse problem, making the training of VQ-VAE unstable. Instead of increasing the codebook size, RQ uses a fixed size of codebook to recursively quantize the feature map in a coarse-to-fine manner. Namely, at each iteration, it will calculate the distance of previous residual and its corresponding code as the new residual. After D iterations of RQ, the feature map is represented as a stacked map of D discrete codes. Since RQ can compose as many vectors as the codebook size to the power of D, RQ-VAE can precisely approximate a feature map, while conserving the information of the encoded image without a huge codebook as, RQ with depth D and codebook size K has the same partition capacity as VQ with K^D codes. Thus, RQ-VAE can further reduce the spatial resolution of the quantized feature map than previous studies.


It also proposes RQ-Transformer to predict the codes extracted by RQ-VAE. For the input of RQTransformer, the quantized feature map in RQ-VAE is converted into a sequence of feature vectors.
Then, RQTransformer predicts the next D codes to estimate the feature vector at the next position.It also proposes two training techniques for RQ-Transformer, soft labeling and stochastic sampling for the codes of RQ-VAE to further improve the performance by resolving the exposure bias in the training of RQ-Transformer.


### VQ-GAN-CLIP
Uses VQ-GAN to create latent space and create images. Using CLIP to further improve the output image. During the training phase, the loss to be minimized is the CLIP loss of augmented output images to the text prompt.It can both perform image generation and pertubation. 


### Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors



use two modified VQVAE to encode and decode the image and scene tokens with explicit losses targeted at specific image regions correlated with human perception and attention, such as faces and salient objects. The first VQ-VAE is used for scene segmentation, the other is modified to increase awareness and perceptual knowledge of objects defined as “things” in the panoptic segmentation categories as well as the awarness and perceptual knowledge of face segementation categories. For the face part, a specialized face-embedding network is utilized. For the object awareness,  it employs a pre-trained VGG network trained on ImageNet, and introduce a feature-matching loss representing the perceptual differences between the object crops of the reconstructed and ground-truth images.

The losses contribute to the generation process by emphasizing the specific regions of interest and integrating domain-specific perceptual knowledge in the form of network feature-matching.


The generative model is an autoregressive model. The input of it, in addition to the conventional use of text and image tokens, implicit conditioning over optionally controlled scene tokens derived from segmentation maps can be introduced. During inference, the segmentation tokens are either generated independently by the transformer or extracted from an input image, providing freedom to impel additional constraints over the generated image.


### UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes

introduce UViM, a unified approach capable of modeling a wide range of computer vision tasks. In contrast to previous models, UViM has the same functional form for all tasks; it requires no task-specific modifications which require extensive human expertise. The approach involves two components: (I) a base model (feedforward) which is trained to directly predict raw vision outputs, guided by a learned discrete code and (II) a language model (autoregressive) that is trained to generate the guiding code


To build a unified vision model we start from a base model which directly maps task inputs to its outputs. Then use VQ-VAE to generate guiding codes with the ground-truth label, adding the guiding codes to the base model and train base model and VQ-VAE jointly and end-to-end by minimizing a reconstruction loss. It also adapts the classic Linde-Buzo-Gray splitting algorithm to VQ-VAE’s dictionary learning procedure. Specifically, if throughout the training process, an unused embedding is detected, the most frequently used embedding will be taken and split into two new embeddings by applying a tiny noise, and consequently replacing the unused one.

Then at stage 2, learning to model the guiding code. It will model the guiding code using the input x. The training data is a collection of input-output pairs (x, Ω(y)), where Ω is the fixed restricted oracle trained from the stage I. Note, that this task is equivalent to many standard NLP problems (except the input is an image) and there is a vast number of research and tools to tackle it. Training is performed end-to-end with gradient-based optimization. It also explore a code dropout mechanism to affect the code complexity so that base model learns to not rely on any individual code too heavily and the code becomes more robust.

### STAR: A Structure-aware Lightweight Transformer for Real-time Image Enhancement

STAR is formulated to capture long-range dependencies between image patches, which naturally and implicitly captures the structural relationships of different regions in an image. STAR is a general architecture that can be easily adapted to different image enhancement tasks.