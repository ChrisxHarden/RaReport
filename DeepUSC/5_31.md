# May 31 Report


## TODO:
1. Read the papers in the Quick Note Section, discover what are the papers that need attention and to be mentioned in related work.
2. Based on VQ-VAE, VQ-GAN and VQ-diffusion, trace the papers that cite them and filter 
## Done:
1. Understood VQ-VAE,VQ-GAN
2. Conclusion on VQ-GAN:
This part is listed in Conclusion of papers, you can check whether they meet the criterion and then I will put them on overleaf projects.
## Doing:
1. Reading VQ-Diffusion
2. based on vq-vae, search through all the papers that cited it and find useful ones.




----------------------

## quetions to be asked:

1. During reading the vq-diffusion: latent-space model is well-suited for the task of textto-image generation. Should I look into recent works? Can you give me some examples?
2. Starting from VQ-GAN and explore the articles that cited it, is this a good way?
3. Are latent variable models to be considered as generative models trained on latent space?
4. The forward process yields a sequence of increasingly noisy latent variables producing pure noise after a fixed number of timesteps. Starting from this noise result, the reverse process gradually denoises the latent variables towards the desired data distribution by learning the conditional transit distribution. should I consider diffusion models as generative models trained on latent space?
   



------------------------

## Conclusion on papers:

### VQ-VAE
Have read VQ-VAE, but it's not in the time range.

But it's a root for nowadays generative models that utilize latent space, I can put a conclusion on it if it can be reckoned as valid related work.


### Taming Transformers for High-Resolution Image Synthesis（VQ-GAN）
#### Description:
This article aims at synthesizing high-resolution pictures using transformer. Due to the high computational cost of directly tranforming the image into pixel sequence, the article utilized the VQ-VAE structure to efficiently learn a codebook of context-rich visual part with some modification. The encoder extracts an encoding $Z_\hat$ which is then quantized to $Z_q$ using the closest codebook entry. The decoder can then reconstruct the image starting from the quantization. What's different lies in the process of learning the codebook, they change the MSE loss in the VQ-VAE's loss function into a perceptual loss and utilize an adversarial approach to ensure that the dictionary of local parts captures perceptually important local structure to alleviate the need for modeling low-level statistics with the transformer architecture. 

The synthesizing part will take place after the codebook is learnt. The input image will be presented as the sequence of indicies of the encoded image in the codebook. An autoregressive transformer can then be utilized to generate the following code, turning image-synthesis problem into autoregressive next-index problem. 

During synthesizing, it can also take in user's command to do conditioned synthesizing.
#### similarity
1. Utilizing VQ to simplify the structure of picture.
2. Training a generative model on latent space, in this case it's the codebook

#### Difference
1. used in image synthesizing.
2. ignore the existence of unidirection bias and error propaganda.
   
### VQ-diffusion (reading)




--------------------------
## Quick Notes
1. VQ-VAE
2. vq-gan
3. parti
4. clip
5. high resolution image synthesis with latent diffusion models
6. latent point diffusion models for 3d shape generation
7. latent space diffusion models of cryo-em
8. symbolic music generation with diffusion models


1. vq-vae2
2. jukebooks
3. dall-e
4. rq-va