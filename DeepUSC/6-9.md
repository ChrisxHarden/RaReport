# June 9 report
## TODO
1. Finish all the work including reorganize the conclusion by Monday
2. have a look at CLIP related works.
## DONE
1. 37 in overleaf
## Doing
1. M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis via Non-Autoregressive Generative Transformers
2. MAKE-A-VIDEO: TEXT-TO-VIDEO GENERATION WITHOUT TEXT-VIDEO DATA
3. VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature
----------------------
## Questions
---------------------
## Quick notes 
### Diffsound: Discrete Diffusion Model for Text-to-sound Generation
1. In this study, we investigate generating sound conditioned on a text prompt and propose a novel textto-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a tokendecoder, and a vocoder. The framework first uses the tokendecoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated melspectrogram into a waveform. We found that the token-decoder significantly influences the generation performance. Thus, we focus on designing a good token-decoder in this study. We begin with the traditional autoregressive (AR) token-decoder, which has shown state-of-the-art performance in previous sound generation works. However, the AR token-decoder always predicts the melspectrogram tokens one by one in order, which may introduce the unidirectional bias and accumulation of errors problems.
Moreover, with the AR token-decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR token-decoders, we propose a non-autoregressive token-decoder based on the discrete diffusion model, named Diffsound
1. the Diffsound model predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained by iteration
2. first uses autoregressive (AR) decoder to generate a mel-spectrogram conditioned on a one-hot label or a video and then employs a vocoder (e.g. MelGAN [6]) to transform the generated mel-spectrogram into waveform. To improve the generation efficiency, they propose to learn a prior in the form of the Vector Quantized Variational Autoencoder (VQ-VAE) codebook [7], which aims to compress the mel-spectrogram into a token sequence. With VQ-VAE, the mel-spectrogram generation problem can be formulated as predicting a sequence of discrete tokens from the text inputs. Inspired by [3], [4], we propose a text-to-sound generation framework, which consists of a text encoder, a VQ-VAE, a token-decoder, and a vocoder.
3. employed the pretrained BERT [39] and the text encoder of a pretrained Contrastive Language-Image Pre-Training (CLIP) model [40] to extract the text features
4. there is no direct correspondence between text and sound in the sound generation task. To this end, the text-to-sound task needs to extract the event information from the text input and then generate the corresponding events. Considering a sound may consist of multiple events and each event has its own unique characteristics. We propose to use the VQ-VAE to learn a codebook to encode the characteristic of events, and then generate the mel-spectrogram based on the codebook
5. Thus, the mel-spectrogram generation problem transfers to predicting a sequence of tokens.
6. 
### ShapeFormer: Transformer-based Shape Completion via Sparse Representation
1. , a transformer-based network that produces a distribution of object completions, conditioned on incomplete, and possibly noisy, point clouds. The resultant distribution can then be sampled to generate likely completions, each exhibiting plausible shape details while being faithful to the input.
2. introduce a compact 3D representation, vector quantized deep implicit function (VQDIF), that utilizes spatial sparsity to represent a close approximation of a 3D shape by a short sequence of discrete variables.
3. introduce Vector Quantized Deep Implicit Functions (VQDIF), a novel 3D representation that is both compact and structured, that can represent complex 3D shapes with acceptable accuracy, while being rather small in size. The core idea is to sparsely encode shapes as sequences of discrete 2-tuples, each representing both the position and content of a non-empty local feature. These sequences can be decoded to deep implicit functions from which high-quality surfaces can subsequently be extracted.
4. 
### SpaText: Spatio-Textual Representation for Controllable Image Generation
1. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels.
2. SpaText — a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. 
3. given a global text prompt that describes the entire image, and a spatio-textual scene that specifies for segments of interest their local text description as well as their position and shape, a corresponding image is generated
4. providing the user with more control over the regions they care about, leaving the rest for the machine to figure out.
5. implement it on two state-of-theart types of text-to-image diffusion models: a pixel-based model (DALL·E 2 [51]) and a latent-based model (Stable Diffusion [56]).
6. use a pre-trained panoptic segmentation model [69] along with a CLIP [49] model.
7. during training we use the image encoder CLIPimgto extract the local embeddings using the pixels of the objects that we want to generate (because the local text descriptions are not available), whereas during inference we use the CLIP text encoder CLIPtxtto extract the local embeddings using the text descriptions provided by the user.


### GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions