## TODO
1. To be determined after the meeting.

## Done
1. finish reading the paper.
2. finish reading the readme part of all repos.
## Doing
1. figuring out the target




## Questions:
1. My questions about the target:
    explain the automatic construction of a memory mapped dataset (Understand the code in AUTO-DS and explain it?)

    efficiency in training on the feature space as opposed to the original image space to evaluate the performance of a learning algorithm(for continual learning?).

    We will evaluate of memory mapped (all memory based dataset or only ours) performance as opposed to h5 or file i/o.


2.  Is "automatically construct a sequence of tasks" the reason why we say "automatic construction of a memory mapped dataset"?
   
3. Currently I formulate my directions to read the related papers are: 1. Dataset construction and evaluation 2. Continual learning papers but focus on how they deal with multiple datasets. 3. How to project different dataset into a feature space with same dimension. Are they correct?


4. My understanding of our method's strength: 1. it projects all kinds of dataset into a feature space with same dimension so it's applicable for continual learning and it can improve training speed. 2. It can provide multi-modality data. Are they correct?
 




## conclusion paper drafts



### Batch Model Consolidation: A Multi-Task Model Consolidation Framework
1. Continual Learning (CL) has allowed deep learning models to learn in a real world that is constantly evolving, in which data distributions change, goals are updated, and critically, much of the information that any model will encounter is not immediately available [2]. Current approaches in CL provide a trade-off to the stability-plasticity dilemma [3] where improving performance for a novel task leads to catastrophic forgetting.
2. Continual Learning benchmarks are composed of a limited number of tasks and with tasks of non-distinct domains
3. Previous approaches in Continual Learning suffer significant performance degradation when faced with a large number of tasks, or tasks from diverse domains
4. Batch Model Consolidation (BMC), a Continual Learning framework that supports distributed training on multiple streams of diverse tasks, but also improves performance when applied on a single long task stream. Our method trains and consolidates multiple workers that each become an expert in a task that is disjoint from all other tasks. In contrast, for Federated Learning the training set is composed of a single task of heterogeneous data
5. First, during the regularization phase, a set of expert models is trained in new tasks in parallel with their weights regularized to a base model
6. during the consolidation phase the expert models are combined into the base model in a way that better retains the performance on the current tasks of all experts and all previously learned tasks.
7. it provides a better approximation to the multi-task gradient of all tasks from all expert models
8.  We propose Batch Model Consolidation (BMC) to support CL for training multiple expert models on a single task stream composed of tasks from diverse domains.
9.  We extend BMC for a distributed learning framework where we train multiple expert models on disjoint task streams.
10.  We propose a stability loss to reduce forgetting that is applied between expert models and a base model.
Lastly, a batched distillation loss combines multiple expert models to update a single base model in a single incremental step.