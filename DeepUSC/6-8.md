# June 8 report
## TODO
1. formulating the conclusion after all 40 finished
## DONE
1. 33 in overleaf, working on 35 by tonight.
## Doing
1. Diffsound: Discrete Diffusion Model for Text-to-sound Generation
2. finding more related papers as some papers I thought useful turns out to be irrelevant
----------------------
## Questions





---------------------
## Conclusion drafts


### Cogview2
1. masks various types of tokens in the sequence of text and image tokens, and learns to predict them autoregressively
2.  Cross-Modal general Language Model
3.  First, we generate a batch of low-resolution images (20 × 20 tokens in CogView2) using the pretrained CogLM, and then (optionally) filter out the bad samples based on the perplexity of CogLM image captioning
4.  The generated images are mapped into 60 × 60-token images by a direct super-resolution module fine-tuned from the pretrained CogLM. We use local attention implemented by our customized CUDA kernel to reduce the training expense.
5.  These high-resolution images are refined via another iterative super-resolution module finetuned from the pretrained CogLM. Most tokens are re-masked and re-generated in a local parallel autoregressive (LoPAR) way, which is much faster than the original autoregressive generation.
6.  , during pretraining the masked patch prediction task trains CogLM to handle bidirectional context, making it easy to adapt to bidirectional tasks, such as direct and iterative super-resolution
7.  the hierarchical design allows us to care only about the local coherence at a high-resolution level. In this way, the local attention can be leveraged to reduce the training expense
8.  local parallel autoregressive generation can reduce model run times 
9.  A possible drawback of pure NAR methods is that tokens sampled at the meantime might lead to global inconsistency in later steps during the generation of complex scenes. Our method introduces a hierarchical design to combine the consistency merit of autoregressive models and the speed advantage of NAR methods.
10. ogLM takes as input a concatenation of text and images tokenized by icetk 1(See § 3.2), whose dictionary contains 20,000 image tokens and 130,000 text (both Chinese and English) tokens
11. fine-tune the pretrained CogLM into an encoder-decoder architecture for Direct super-resolution
12. In autoregressive generation, the sampling strategy over the predicted distribution of the tokens is crucial. Top-k or top-p (nucleus) sampling [14] are the most common strategies, but suffer from an incomplete truncation problem.group the 20,000 tokens into 500 clusters via Kmeans [18] based on their vectors in VQVAE. During sampling, we first sample a cluster using top-k sampling based on the sum of probabilities of tokens in the clusters, and then sample in the cluster.

### Muse: Text-To-Image Generation via Masked Generative Transformers
1. text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models
2. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens.
3. compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding.
4. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model
5. First, we have a pair of VQGAN “tokenizer” models (Esser et al., 2021b), which can encode an input image to a sequence of discrete tokens as well as decode a token sequence back to an image. We use two VQGANs, one for 256x256 resolution (“low-res”) and another for 512x512 resolution (“high-res”). Second, we have a base masked image model, which contains the bulk of our parameters. This model takes a sequence of partially masked low-res tokens and predicts the marginal distribution
6. for each masked token, conditioned on the unmasked tokens and a T5XXL text embedding. Third, we have a “superres”transformer model which translates (unmasked) low-res tokens into high-res tokens, again conditioned on T5-XXL text embeddings.
7. Pre-trained Text Encoders
8. We build our encoder and decoder entirely with convolutional layers to support encoding images from different resolutions.
9. train two VQGAN models: one with downsampling ratio f = 16 and the other with downsampling ratio f = 8. We obtain tokens for our base model using the f = 16 VQGAN model on 256×256 pixel images, thus resulting in tokens with spatial size 16 × 16. We obtain the tokens for our super-resolution model using the f = 8 VQGAN model on 512 × 512 images, and the corresponding token has spatial size 64 × 64.
10. base model is a masked transformer(Vaswani et al., 2017; Devlin et al., 2018), where the inputs are the projected T5 embeddings and image tokens.
11. leave all the text embeddings unmasked and randomly mask a varying fraction of image tokens (see Section 2.6) and replace them with a special [MASK]token (Chang et al., 2022). We then linearly map image tokens into image input embeddings of the required Transformer input/hidden size along with learned 2D positional embeddings. Following previous transformer architecture (Vaswani et al., 2017), we use several transformer layers including self-attention block, cross-attention block and MLP block to extract features. At the output layer, an MLP is used to convert each masked image embedding to a set of logits (corresponding to the VQGAN codebook size) and a cross-entropy loss is applied with the ground truth token label as the target. At training, the base model is trained to predict all masked tokens at each step. However, for inference, mask prediction is performed in an iterative manner which significantly increases quality.
12. employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment
13. e use of parallel decoding to predict multiple output tokens in a single forward pass. The key assumption underlying the effectiveness of the parallel decoding is a Markovian property that many tokens are conditionally independent given other tokens. Decoding is performed based on a cosine schedule (Chang et al., 2022) that chooses a certain fixed fraction of the highest confidence masked tokens that are to be predicted at that step. These tokens are then set to unmasked for the remainder of the steps and the set of masked tokens is appropriately reduced.


### Diffusion Autoencoders: Toward a Meaningful and Decodable Representation
1. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction.
2. Finding a meaningful representation that is decodable requires capturing both the high-level semantics and low-level stochastic variations. Our key idea is to learn both levels of representation by utilizing a learnable encoder for discovering high-level semantics and utilizing a DPM for decoding and modeling stochastic variations. In particular, we use our conditional variant of the Denoising Diffusion Implicit Model (DDIM) [47] as the decoder and separate the latent code into two subcodes. The first “semantic” subcode is compact and inferred with a CNN encoder, whereas the second “stochastic” subcode is inferred by reversing the generative process of our DDIM variant conditioned on the semantic subcode.
3. can be used to generate image samples by fitting another DPM to the semantic subcode distribution
4. 



### NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion

1. VideoGPT [48] and VideoGen [51] extend convolutions in the VQ-VAE encoder from 2D to 3D and train a video-specific representation. However, this fails to share a common codebook for both images and videos
2. using 2D VQ-GAN to encode each frame of a video can also generate temporal consistency videos and at the same time benefit from both image and video data.
3. 