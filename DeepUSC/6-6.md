# June 6 Report

## TODO
1. Search for related works in flow based generator and flow regularization
2. A disentangling invertible interpretation network for explaining latent representations.
## DOne
1. Listed in the overleaf, 23 in total
## Doing
1. GAUDI: A Neural Architect for Immersive 3D Scene Generation



---------------------
## Questions
1. Techniques to create a compressed latent space other than Quantization?
2. I think photo compression is not a valid way to create latent space?  



---------------------------
## Conclution Drafts

### Parti
use vit-vqgan to encode the image

1.  train a ViT-VQGAN-Small configuration (8 blocks, 8 heads, model dimension 512, and hidden dimension 2048 as shown in Table 2 of [21], with about 30M total parameters), and learn 8192 image token classes for the codebook
2.  To further improve visual acuity of the reconstructed images after second-stage encoder-decoder training, we freeze the tokenizer’s encoder and codebook, and finetune a larger-size tokenizer decoder
3.  employ a simple super-resolution module on top of the image tokenizer, shown in Figure 4. Stacked convolutional layers with residual connections are used as the super-resolution network module following WDSR
4.  a standard encoder-decoder Transformer model is trained at the second stage, by treating text-to-image as a sequence-to-sequence modeling problem.takes text as input and is trained using next-token prediction of rasterized image latent codes generated from the first stage image tokenizer. For text encoding, we build a sentence-piece model [32, 33] of vocabulary size 16,000 on a sampled text corpus from the training data (Section 4.1). Image tokens are produced by a learned ViT-VQGAN image tokenizer (see Section 2.1). At inference time, the model samples image tokens autoregressively, which are later decoded into pixels using the ViT-VQGAN decoder.
5.  configure the Transformers following previous practice of those in scaling language models with default expansion ratio of 4× in MLP dimensions. We double the number of heads when the model dimension is doubled.
6.  Text Encoder Pretraining
7.  Classifier-Free Guidance and Reranking for better quality



### ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis
1. To tackle the difficult problem of modeling a highly complex distribution p(x) of high-dimensional images x, we (i) introduce bidirectional context into an otherwise unidirectional autoregressive factorization of p(x) as in Eq. (1) and (ii) reduce the difficulty of the learning problem with a hierarchical approach.
2. This introduces a coarse-to-fine hierarchy of image representations x0:T:= (xt)T t=0, such that an xt−1is modeled conditioned on xt, i.e. xt−1∼ pt−1 θ(xt−1|xt) and defines a reverse Markov Chain for x =: x0as pθ(x0) = pT θ(xT)?T t=1pt−1 θ(xt−1|xt). Since our goal is to approximate the original distribution p(x) with pθ(x0), we introduce a forward Markov Chain, qθ(x1:T|x0) =?T t=1qt θ(xt|xt−1), to obtain a tractable upper bound on the Kullback-Leibler (KL) divergence between p and pθ
3. use vq-gan for first step
4. first learn a compressed, discrete image representation x1 and subsequently our generative ImageBART model reverts a fixed multinomial diffusion process via a Markov Chain, where the individual transition probabilities are modeled as independent autoregressive encoder-decoder models. This introduces a coarse-to-fine hierarchy such that each individual AR model can attend to global context from its preceding scale in the hierarchy.


### Jukebox

1. generates music with singing in the raw audio domain. tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable.
2. uses a hierarchical VQ-VAE architecture
3. use random starts to deal with codebook collapse:when the mean usage of a codebook vector falls below a threshold, we randomly reset it to one of the encoder outputs from the current batch.
4. To maximize the amount of information stored at each level, we simply train separate autoencoders with varying hop lengths. Discrete codes from each level can be treated as independent encodings of the input at different levels of compression.
5. use Transformers with sparse attention to generate
6. At each level, we use Transformers over the same context length of discrete codes, which correspond to increasing the raw audio length with larger hop lengths, and modeling longer temporal dependencies at the higher levels while keeping the same computational footprint for training each level.

### VECTOR-QUANTIZED IMAGE MODELING WITH IMPROVED VQGAN
1. using vit-vqgan to encode

2. train a Transformer model to predict rasterized 32×32 = 1024 image tokens autoregressively,

3. For unconditional image synthesis or unsupervised learning, we pretrain a decoder-only Transformer model to predict the next token. For class-conditioned image synthesis, a class-id token is prepended before the image tokens.

4. replacing the CNN encoder and decoder with Vision Transformer (ViT),less constrained by the inductive priors imposed by convolutions. Furthermore, ViT-VQGAN yields better computational efficiency on accelerators, and produces higher quality reconstructions,
5. introduce a linear projection from the output of the encoder to a lowdimensional latent variable space for code index lookup
6. apply ?2normalization on the encoded latent variables ze(x) and codebook latent variables
7. use a combination of logit-laplace loss, ?2loss, perceptual loss (Johnson et al., 2016; Zhang et al., 2018) based on VGG network (Simonyan & Zisserman, 2014) and GAN loss with architecture of StyleGAN discriminator
8. With a pretrained generative Transformer model, unconditional image generation is achieved by simply sampling token-by-token from the output softmax distribution.

### Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation
1. take text-to-image generation to the realm of text-guided Image-to-Image (I2I) translation, where an input image guides the layout (e.g., the structure of the horse in Fig. 1), and the text guides the perceived semantics and appearance of the scene
2. consider StableDiffusion [37], a state-of-the-art pre-trained and fixed text-to-image LDM mode
3. fine-grained control over the generated structure can be achieved by manipulating spatial features inside the model during the generation process.as spatial features extracted from intermediate decoder layers encode localized semantic information and are less affected by appearance information  and self-attention representing the affinities between the spatial features, allows to retain fine layout and shape details.
4. devise a simple framework that extracts features from the generation process of the guidance image IGand directly injects them along with P into the generation process of I∗, requiring no training or fine-tuning
5. Our approach is applicable for both text-generated and real-world guidance images, for which we apply DDIM inversion [45] to get the initial xG T.
6. Given the target prompt P, the generation of the translated image I∗is carried with the same initial noise
7. Feature injection
8. At each step t of the backward process, we extract the guidance features {fl t} from the denoising step,These features are then injected into the generation of I∗, i.e., in the denoising step of x∗t, we override the resulting features
9. we inject features in deeper layers, the structure is better preserved, yet appearance information is leaked into the generated image (e.g., shades of the red tshirt and blue jeans are apparent in Layer 4-11). To achieve a better balance between preserving the structure of IGand deviating from its appearance, we do not modify spatial features at deep layers, but rather leverage the self-attention layers
10. 

### MoVQ: Modulating Quantized Vectors for High-Fidelity Image Generation
1. propose to incorporate the spatially conditional normalization to modulate the quantized vectors so as to insert spatially variant information to the embedded index maps, encouraging the decoder to generate more photorealistic images
2. use multichannel quantization to increase the recombination capability of the discrete codes without increasing the cost of model and codebook.
3. adopt a Masked Generative Image Transformer (MaskGIT) to learn an underlying prior distribution in the compressed latent space, which is much faster than the conventional autoregressive model.
4. The quantization operator is lossy [31], and similar patches are often embedded as the same codebook indices, resulting in a repeated artifact when they are synthesized through existing decoder architectures (see Fig. 3 (a)). As opposed to existing methods that directly feed discrete feature maps into the decoder as activations, in this work, we introduce a new spatially conditional normalization layer to propagate the embedded contents to the activations.
The key motivation behind this is to add spatial variants to the discrete maps, such that the model can generate plausible and diverse results, even for the same quantization index in neighboring regions.

5. Unlike existing quantizers that map an image into a single-channel index map, here we convert it into a multichannel index map with the shared codebook to further improve the image quality,      Due to this reduced dimension of each codevector, the total computational cost is similar
6. use maskgit's setting for stage2 and generating model

### 


-----------------------

## Quick Notes
1. cogview
2. Network-to-network translation with conditional invertible neural networks
3. A deep hierarchical variational autoencoder.
4. Styleflow: Attribute-conditioned exploration of stylegan generated images using conditional continuous normalizing flows.
5. A disentangling invertible interpretation network for explaining latent representations.
6. hifi compression
7.  flow based generator
8.  High-quality pluralistic image completion via code shared vqgan
9.  