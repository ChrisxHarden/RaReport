# May 30 Report


## TODO:

1. transform the proper papers' summary on overleaf
2. look for more papers in vector quantization and summarize them in markdown first.
3. change the template of overleaf.
   
## Done:
The paper summaries listed in section: Conclusion on papers
## Doing:

searching for and reading papers in latent space generating models. 

currently reading: DLFormer: Discrete Latent Transformer for Video Inpainting


----------------------

### quetions to be asked:
1. as we always learn the gan models by first paramertize the input using a latent variable and thus the model is learnted in the latent space, should I look further into gans? (they are important)

2. only training a generative model on latent space should be listed in the related works or other models or utilizing latent space for improved performance also count.

------------------------

## Conclusion on papers:

### Detailed:

#### Latent Space Oddity: on the Curvature of Deep Generative Models
1. Description: Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear “generator” function that maps latent points into the input space. This paper claims that the latent space will have curvature and when we estimate generative models we should use riemannian metric to measure the distance. By using this metric, the performance and variance improved. In the paper's experiment, using the riemannian metric will bring forward more meaningful distance , better interpolation result and more data-driven and dependent as the distance of riemannian will increase due to the the increase of variance or uncertainty




#### Score-based Generative Modeling in Latent Space
1. Score-based generative models usually applied directly in data space and often require thousands of network evaluations for sampling.
2. Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework
   map the input data to latent space and apply the score-based generative model there. The score-based model is then tasked with modeling the distribution over the embeddings of the data set,i.e. the latent space. Novel data synthesis is achieved by first generating datapoints in latent space  via drawing from a simple base distribution followed by iterative denoising, and then transforming this datapoint in latent space via a decoder to data space
3. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling.



Are those good ones? If they are, i will update on overleaf with more scientific tone.




---------------------------

### Not sure

#### PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models
this paper aims at doing super resolution of photos by searching for the photo that can downscale to the input low-resolution photo correctly. The manifold to be searched is given by the latent space of a generator model. This paper is more like describing a new method for photo upsampling rather than train a generative model on latent space

#### Learining deep latent space for multi-label classification
I dont' think it's a generative model.but I still gonna see it after the meeting if it worth to be looked at.


#### learning latent space energy based prior model
learn energy-based model (EBM) in the latent space of a generator model, so that the EBM serves as a prior model that stands on the top-down network of the generator model  

#### Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling
generate 3-d objects from latent probabilistic space.


#### stylegan
1. The generator starts from a learned constant input and adjusts the “style” of
the image at each convolution layer based on the latent
code, therefore directly controlling the strength of image
features at different scales. 
2. embeds the input latent code into an intermediate latent space



#### styleclip:
1. description: modifying the latent space of stylegan to manipulate the generated and real photo via text.

conclusion: it's mainly about how to utilize the latent space learned by stylegan and provide more degree of freedom in terms of style transformation.


