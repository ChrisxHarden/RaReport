## TODO
1. ARDA: Automatic relational data augmentation for machine learning
2. AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data
## DONE
1. Multi-modal Alignment using Representation Codebook
2. HAA500: Human-Centric Atomic Action Dataset with Curated Videos (The dataset itself is impressive, the curation goal is practical, but the curation method itself is human-labor which is not relevant to our task)
## DOING
1. Reading "Data Management for Machine Learning: A Survey", and since it's a survey, I don't think I need to summary this or compare with our method, it mainly serves as a guide. 


## QUESTIONS
1. Is reading some single dataset paper, like HAA500 I found in ICCV 2021's accepted papers, useful?  They may come up with a dataset and we can at least include it in our method. But some of them are just human curated or manually searched and created which is not what our method meant for. 
### Drafts
Representation:
1. treat image and text as two “views” of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centers (codebook).
We contrast positive and negative samples via their cluster assignments while simultaneously optimizing the cluster centers. To further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other.
1. The main focus of our work is on the feature alignment stage, which is challenging due to the fact that image and text inputs have very different characteristics.
2. encode image and text into a joint vision-language embedding space and learn the alignment by contrasting their prototype assignments. The codebook can also be interpreted as underlying feature distribution for the paired data
3. 