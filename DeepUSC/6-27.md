## Todo
1. Read the code in rtdl repo to know how they preprocess the tabular data
## Done
1. Finish the collection of the datasets.
2. had a quick look at the paper:"Revisiting Pretraining Objectives for Tabular Deep Learning",but it didn't mention how it preprocess the data.
## Doing
1. Reading through the tabular-dl-pretrain-objectives repo to have a look on how they deal with tabular data.

## Questions
1. From the datasets I have collected, some of them are not traditional tasks like regression and classification. For example, this is clustering, this is predicting missing value from given table, and this is to blend existing files to improve prediction score. Should we tranform these datasets into autodataset as well?
   
2. should I make the tabular dataset also memory mapped? Since some datasets are huge, if we need to perform memory mapped, I am thinking about feature reduction or distillation.
   
3. I am going to follow the auto-ds repo's style and expand it to tabular datasets.
   
4. possible curation: 




5. 
   1. Dealing with nan values and rare values, 
   2. outliner detection, 
   3. filling in the missing value, 
   4. tranform the features that are not number or category into number or categorical features(like date and time),
   5. using the feature tokenizer to tokenize the processed feature.
   6. should be able to deal with imbalanced dataset
   7. Auto download and verify 

