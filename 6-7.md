# June 7 report
## TODO
1. formulating the conclusion after all 40 finished

## DONE
1. 29 in overleaf, working on 32 by tonight
## Doing
1. CogView2
2. Muse: Text-To-Image Generation via Masked Generative Transformers
3. Deep Learning in Latent Space for Video Prediction and Compression
----------------------
## Questions


---------------------
## Conclusion drafts

### GAUDI: A Neural Architect for Immersive 3D Scene Generation
1. GAUDI, a generative model capable of capturing the distribution of complex and realistic 3D scenes that can be rendered immersively from a moving camera.
2. first optimize a latent representation that disentangles radiance fields and camera poses. This latent representation is then used to learn a generative model that enables both unconditional and conditional generation of 3D scenes.
3. we map each trajectory (i.e. a sequence of posed images from a 3D scene) into a latent representation that encodes a radiance field (e.g. the 3D scene) and camera path in a completely disentangled way. We find these latent representations by interpreting them as free parameters and formulating an optimization problem where the latent representation for each trajectory is optimized via a reconstruction objective.
4. Interpreting the latent representation of each trajectory as a free parameter also makes it simple to handle a large and variable number of views for each trajectory rather than requiring a sophisticated encoder architecture to pool across a large number of views.
5. scale 3D scene generation to thousands of indoor scenes containing hundreds of thousands of images, without suffering from mode collapse or canonical orientation issues during training.
6. introduce a novel denoising optimization objective to find latent representations that jointly model a radiance field and the camera poses in a disentangled manner.
7. The task is to find a latent representation for each input trajectory
8. trajectory xiis defined as a variable length sequence of corresponding RGB, depth images and 6DOF camera poses
9. latent representation z = [zscene, zpose] for each example x 2 X that represents the scene radiance field and pose in separate disentangled vectors.
10. To obtain this latent representation we take an encoder-less view
11. The camera pose decoder network c (parameterized by ✓c), is responsible for predicting camera posesˆTs2 SE(3) at the normalized temporal position s 2 [?1, 1] in the trajectory, conditioned on zposewhich represents the camera poses for the whole trajectory.
12. The scene decoder network d (parameterized by ✓d), is responsible for predicting a conditioning variable for the radiance field network f. This network takes as input a latent code that represents the scene zsceneand predicts an axis-aligned tri-plane representation [41, 5] W 2 R3⇥S⇥S⇥F. Which correspond to 3 feature maps [Wxy, Wxz, Wyz] of spatial dimension S ⇥ S and F channels, one for each axis aligned plane: xy, xz and yz.
13. The radiance field decoder network f (parameterized by ✓f), is tasked with reconstructing image level targets
14. while latents z are optimized for each example x independently, the parameters of the networks ✓d, ✓c, ✓fare amortized across all examples x 2 X.
15. As opposed to previous auto-decoding approaches [2, 39], each latent z is perturbed during training with additive noise that is proportional to the empirical standard deviation across all latents, z = z + ?N (0, std(Z)), inducing a contractive representation [50].
16. We use a small ? > 0 value to enforce a latent space in which interpolated samples (or samples that contain small deviations from the empirical distribution, as the ones that one might get from sampling a subsequent generative model) are included in the support of the decoder function, sacrificing a small cost in reconstruction fidelity.
17. optimize parameters ✓d, ✓f, ✓cand latents z 2 Z with two different losses.
18. In order to model p(Z) we employ a Denoising Diffusion Probabilistic Model (DDPM) [17], a recent score-matching [18] based model that learns to reverse a diffusion Markov Chain with a large but finite number of timesteps. 
19. We start by sampling zT⇠ N (0, I) and iteratively apply ✏✓pto gradually denoise zT, thus reversing the diffusion Markov Chain to obtain z0. We then feed z0as input to the decoder architecture (cf. Fig. 2) and reconstruct a radiance field and a camera path.


### VideoGPT: Video Generation using VQ-VAE and Transformers
1. uses VQVAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention.
2. A simple GPTlike architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings.
3. employs 3D convolutions and transposed convolutions (Tran et al., 2015) along with axial attention (Huang et al., 2019; Ho et al., 2019b) for the autoencoder in VQ-VAE,

### Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE
1. Hierarchical VQ-VAE
2. In the second stage, we propose a structural attention module inside the texture generation network, where the module utilizes the structural information to capture distant correlations. We further reuse the VQ-VAE to calculate two feature losses, which help improve structure coherence and texture realism, respectively.
3. multiple-solution inpainting
4. generate multiple structures in an explicit fashion, and then base the inpainting upon the generated structure.
5. hierarchical layout encourages the split of the image information into global and local parts; with proper design, it may disentangle structural features from textural features of an image.
6. propose a twostage model for multiple-solution inpainting.
7. conditional autoregressive network for the distribution over structural features.
8. hierarchical encoder maps ground truth onto structural features sgtand textural features tgt.
9. These features are then quantized to discrete features by two vector quantization layers.
10. the decoder Dvqreconstructs image from these two sets of discrete features. T\
11. the structural features model global information such as shapes and colors, and the textural features model local information such as details and textures.
12. diverse structure generator Gs which uses an autoregressive network to formulate a conditional distribution over the discrete structural features.
13. Similar to the PixelCNN, make two modifications to make it practical for the image inpainting task. First, we stack gated convolution layers to map the input incomplete image Iinand its binary mask M to a condition. The condition is injected into each residual gated convolution layer of the autoregressive network. Second, we use a light-weight autoregressive network by reducing both the hidden units and the residual units to 128 for efficiency.
14. After obtaining the generated structural features ¯ sgen, our texture generator Gt synthesizes the image texture based on the guidance of ¯ sgen
15. The structural features are not only input to the first few layers of Gt, but also input to our structural attention module. The proposed structural attention module borrows distant information based on the correlations of the structural features.
It thus ensures that the synthesized texture is consistent with the generated structure.

### BEIT: BERT Pre-Training of Image Transformers
1. first “tokenize” the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer vit. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEIT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder
2.  Inspired by BERT, we propose a pre-training task, namely, masked image modeling (MIM). 
3.  MIM uses two views for each images, i.e., image patches, and visual tokens
4.  split the image into a grid of patches that are the input representation of backbone Transformer.
5.  we “tokenize” the image to discrete visual tokens, which is obtained by the latent codes of discrete VAE
6.  During pre-training, we randomly mask some proportion of image patches, and feed the corrupted input to Transformer. The model learns to recover the visual tokens of the original image, instead of the raw pixels of masked patches.
7.  perform self-supervised learning and then fine-tune the pretrained BEIT on two downstream tasks, i.e., image classification, and semantic segmentation. 
8.  The input of Transformer is a sequence of image patches {xp i}N i=1. The patches are then linearly projected to obtain patch embeddings Exp i, where E ∈ R(P2C)×D. Moreover, we prepend a special token [S] to the input sequence. We also add standard learnable 1D position embeddings Epos∈ RN×Dto patch embeddings. The input vectors H0= [e[S], Exp i, . . . , Exp N] + Eposis fed into Transformer. The encoder contains L layers of Transformer blocks Hl= Transformer(Hl−1), where l = 1, . . . , L. The output vectors of the last layer HL= [hL [S], hL 1, . . . , hL N] are used as the encoded representations for the image patches,
9.  We randomly mask some percentage of image patches, and then predict the visual tokens that are corresponding to the masked patches.

### BEIT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers
1. we propose to use a semantic-rich visual tokenizer as the reconstruction target for masked prediction, providing a systematic way to promote MIM from pixel-level to semantic-level.
2. propose vector-quantized knowledge distillation to train the tokenizer, which discretizes a continuous semantic space to compact codes. We then pretrain vision Transformers by predicting the original visual tokens for the masked image patches. Furthermore, we introduce a patch aggregation strategy which associates discrete image patches to enhance global semantic representation. 
3. all the reconstruction targets are about, explicitly or implicitly, low-level image elements while underestimating high-level semantics.
4. Vector-Quantized Knowledge Distillation
5. The VQ-KD encoder first converts the input image to discrete tokens according to a learnable codebook.The decoder then learns to reconstruct the semantic features encoded by a teacher model, conditioning on the discrete tokens.
6. Considering the discreteness of tokens, we further introduce a patch aggregation strategy which explicitly encourages the [CLS] token to associate all patches (Gao & Callan, 2021). Such a strategy resolves the issue that MIM put patch reconstruction the first place which diminishes learning global image representations.
7. compute the ?2-normalized distance to find the nearest code while reducing the dimension of codebook embedding space to 32-d. The low-dimensional codebook embeddings are mapped back to higher-dimensional space before being fed to the decoder. Exponential moving average (van den Oord et al., 2017) is employed to update the codebook embeddings. Exponential moving average tends to be more stable for VQ-KD training.

### UNIFIED DISCRETE DIFFUSION FOR SIMULTANEOUS VISION-LANGUAGE GENERATION
1. unified multimodal generation model that can conduct both the ”modality translation” and ”multi-modality generation” tasks using a single model, performing text-based, image-based, and even vision-language simultaneous generation
2. unify the discrete diffusion process for multimodal signals by proposing a unified transition matrix. 
3. design a mutual attention module with fused embedding layer and a unified objective function to emphasise the inter-modal linkages, which are vital for multi-modality generation.


### CogView: Mastering Text-to-Image Generation via Transformers
1. text-to-image pretraining is very unstable under 16-bit precision
2. 

---------------------
## Quick ideas
