## TODO
1. find papers using VQ or similar tricks dealing with NLP tasks.
2. finish categorizing and re-organizing

## Done:
1. The drafts of the related papers. 41 on overleaf
## Doing:
1. Catergorizing and reorganizing the papers.

-------------------------
## Questions:
1. there are VQ based methods that trying to solve the code collapse, should we mention them?
2.  Our next step is to distill our conclusion of papers and fit them into the given introduction?
3.  What is our nlp task? generating sentences? language model? larger-scale language model? Because from what I have read, most of them are not meant for nlp tasks.
4.  Do I need to fill in all the related works?

-------------------------
## conclusion drafts
### M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis via Non-Autoregressive Generative Transformers
1. unify any number of multi-modal controls.
2. In M6-UFC, both the diverse control signals and the synthesized image are uniformly represented as a sequence of discrete tokens to be processed by Transformer
3. M6-UFC adopts non-autoregressive generation (NAR) at the second stage to enhance the holistic consistency of the synthesized image, to support preserving specified image blocks, and to improve the synthesis speed.
4. Conditional image synthesis
5. . Existing works [63, 26, 62] hence typically design separate methods customized for each control modality
6. novel BERT-based two-stage framework to UniFy any number of multi-modal Controls for conditional image synthesis. Concretely, the textual, visual, and preservation control signals, as well as the generated image, are uniformly represented as a sequence of discrete tokens
7. We train M6-UFC via the masked sequence modeling task, which predicts a masked subset of the target image’s tokens conditioned on both the multi-modal control signals and the generation target’s unmasked tokens. During inference, we adopt Mask-Predict, a NAR generation algorithm [16, 21, 7], which predicts all target tokens at the first iteration and then iteratively re-mask and re-predict a subset of tokens with low confidence scores
8. To further improve upon the NAR generation algorithm, we exploit the discriminative capability of the BERT architecture [11, 70] and add two estimators (see Figure 2), where one estimator estimates the relevance between the generated image and the control signals, and the other one estimates the image’s fidelity. The two estimators help improve the quality of the synthesized image
9. can be trained from scratch or initialized with large-scale pre-trained models,
10. follow VQGAN’s design
11. modifies the original BERT model [9] to accommodate any number of multi-modal controls. Similar to BERT, the backbone is a multi-layer bidirectional Transformer encoder, enabling the dependency modeling between all input elements.
12. Each input token’s representation is the sum of the position and token embedding
13. , predicting the masked tokens in the target image conditioned on the controls. A relevance estimator and a fidelity estimator are also trained in the process
14. Progressive Non-Autoregressive Generation
    

### VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature
1. propose VQTTS, consisting of an AM txt2vec and a vocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic feature rather than mel-spectrogram.
2. In avoid of mode collapse where only a little amount of vectors in the codebook are actually used, vq-wav2vec divides the dimension ofˆ Z into 2 groups and quantizes them separately.
3. VQTTS contains two parts, the acoustic model txt2vec which predicts VQ&pros from input phoneme sequence and the vocoder vec2wav which generates waveform from VQ&pros.




### MAKE-A-VIDEO: TEXT-TO-VIDEO GENERATION WITHOUT TEXT-VIDEO DATA
1. learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage.
2. does not need to learn visual and multimodal representations from scratch
3. t does not require paired text-video data
4. the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today’s image generation models
5. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V.
6. Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.
7. First, our architecture breaks the dependency on text-video pairs for T2V generation
8. we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively,
9. our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM
10. based on t2i models. 