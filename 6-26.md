## TODO
1. look for methods to deal with features other than number and categorical
2. getting started on transform the datasets into auto dataset use extracting processed features from ft-transformer
## Done
1. Finish reading ft-transformer
## Doing
1. filling the spreadsheet of kaggle datasets. 


### Questions:
1. For feature extraction of the tabular dataset, I am thinking about 2 methods: first is to use the feature tokenizer to tokenize the num and categorical features, the other is to extract from ft-transformer like what we did using CLIP and VIT. 

2. I am considersing our own auto-dataset implementation on tabular data should be able to deal with features having nearly all data type. But the original FT-transformer's feature tokenizer can only deal with num and categorical features. I need to think about the situation where there are features' data type other than those two.



### Revisiting Deep Learning Models for Tabular Data
1. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works
2. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks.
3. Feature tokenizer only tokenizes number and cat features
4. we are using the feature tokenizer to tokenize the features
5. having feature-biases in it's embedding layer